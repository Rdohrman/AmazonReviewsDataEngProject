package com.kitmenke.spark

import java.util.Date

import org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient
import org.apache.solr.common.SolrInputDocument
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Minutes, Seconds, StreamingContext}

object StreamingApp {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    // Create the context with a 1 second batch size
    val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
    val ssc = new StreamingContext(conf, Seconds(5))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val rdd = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val counts = rdd.flatMap(splitSentenceIntoWords)
      .map(word => (word, 1))
      .reduceByKeyAndWindow((a,b) => a + b, Minutes(30))
    counts.foreachRDD(rdd => {
      rdd.foreachPartition(partition => {
        val connection = new ConcurrentUpdateSolrClient.Builder("http://localhost:8983/solr/gettingstarted").build()
        partition.foreach { case (word, count) => {
          val doc = new SolrInputDocument
          doc.addField("id", word)
          doc.addField("count_l", count)
          doc.addField("last_updated_dt", new Date())
          connection.add(doc)
        }
        }
        connection.commit()
        connection.close()
      })
    })
    ssc.start()
    ssc.awaitTermination()
  }

  def splitSentenceIntoWords(sentence: String): Array[String] = {
    sentence.split(" ").map(word => word.trim().toLowerCase.replaceAll("[^a-z]", "")).filter(p => p.length > 0)
  }
}